{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8: Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification vs. Prediction\n",
    "\n",
    "- **Classification**\n",
    "    - determines categorical class labels\n",
    "    - e.g., safe vs. risky; weather condition\n",
    "- **Prediction**\n",
    "    - models continuous-valued functions\n",
    "- Typical applications\n",
    "    - loan approval, target marketing, medical diagnosis, fraud detection, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification\n",
    "\n",
    "- Step 1: **Learning**\n",
    "    - model construction\n",
    "    - training set, class labels\n",
    "- Step 2: **Classification**\n",
    "    - test set, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Supervised vs. Unsupervised\n",
    "\n",
    "- **Supervised learning (classification)**\n",
    "    - supervision: training data accompanied by class labels\n",
    "    - new data is classified based on training set\n",
    "- **Unsupervised learning (clustering)**\n",
    "    - class labels of training data is unknown\n",
    "    - aims to establish the existence of classes or clusters in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Issues: Evaluation Criteria\n",
    "\n",
    "- **Accuracy**: classification vs. prediction\n",
    "- **Speed**: time to construct / use the model\n",
    "- **Robustness**: handling noise & missing values\n",
    "- **Scalability**: large amounts of data\n",
    "- **Interpretability**: understanding and insight\n",
    "- **Goodness of rules**: e.g., decision tree size, compactness of classification rules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Induction\n",
    "\n",
    "- Basic algorithm (a greedy algorithm)\n",
    "    - **top-down, recursive, divide-and-conquer**\n",
    "    - attribute selection\n",
    "    - attribute split\n",
    "- Stopping conditions\n",
    "    - all samples belong to the same class\n",
    "    - no remaining attributes: **majority voting**\n",
    "    - no samples left"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Attributes\n",
    "\n",
    "- **Discrete**-valued\n",
    "- **Continuous**-valued: split_point\n",
    "- **Discrete**-valued: **binary** tree, splitting_subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Training Set & Decision Tree\n",
    "\n",
    "![Example Training Set & Decision Tree](./img/8.1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attribute Selection Measures\n",
    "\n",
    "- **Information gain** (ID3/C4.5)\n",
    "    - $D$, $m$ classes $\\displaystyle C_i \\quad\\quad p_i = |C_{i,D}|/|D| \\quad\\quad Info(D) = -\\sum_{i=1}^m p_i log_2(p_i)$\n",
    "    - expected information (entropy) needed to classify $D$\n",
    "    - information needed to classify $D$ using $A$\n",
    "        - attribute $A$: $\\displaystyle a_1, a_2, \\ldots, a_v \\quad\\quad Info_A(D) = \\sum_{j=1}^v \\frac{|D_j|}{|D|} \\times Info(D_j)$\n",
    "    - information gain $\\displaystyle \\quad\\quad Gain(A) = Info(D) - Info_A(D)$\n",
    "- Comparison of the three measures\n",
    "    - good results in general but some biases\n",
    "    - **information gain**: multi-valued attributes\n",
    "    - **gain ratio**: unbalanced splits\n",
    "    - **gini index**: multi-valued, equal-sized & pure partitions, not good when number of classes is large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Gain Example\n",
    "\n",
    "![Information Gain Example](./img/8.2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Gain\n",
    "\n",
    "- Continuous-valued attribute $A$\n",
    "- Determine the **best split point** for $A$\n",
    "    - sort $A$ values in increasing order\n",
    "    - consider the midpoint of adjacent values: $(a_i + a_{i+1}) / 2$\n",
    "    - pick the midpoint w/ the minimum $Info_A(D)$\n",
    "- Split: $D_1$: $A \\leq$ split point; $D_2$: $A >$ split point"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gain Ratio (C4.5)\n",
    "\n",
    "- Information gain measure biased towards the attributes with a large number of values\n",
    "    - e.g., customerID, productID\n",
    "- C4.5 (a successor of ID3)\n",
    "    - $\\displaystyle SplitInfo_A(D) = -\\sum_{j=1}^v \\frac{|D_j|}{|D|} \\times log_2\\left(\\frac{|D_j|}{|D|}\\right)$\n",
    "    - select attribute with **maximum gain ratio**\n",
    "        - $\\displaystyle gainRatio(A) = \\frac{Gain(A)}{SplitInfo(A)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gini Index (CART)\n",
    "\n",
    "- **Gini Index**\n",
    "    - $\\displaystyle Gini(D) = 1 - \\sum_{i=1}^m p_i^2$\n",
    "- **Binary split** using attribute A\n",
    "    - $\\displaystyle Gini_A(D) = \\frac{|D_1|}{|D|}Gini(D_1) + \\frac{|D_2|}{|D|}Gini(D_2)$\n",
    "- **Reduction in impurity**\n",
    "    - $\\Delta Gini(A) = Gini(D) - Gini_A(D)$\n",
    "- Select attribute with **largest impurity reduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overfitting & Tree Pruning\n",
    "\n",
    "- **Overfitting of the training data**\n",
    "    - too many branches, reflect anomalies due to noise or outliers\n",
    "    - poor accuracy for unseen data\n",
    "- Tree pruning to avoid overfitting\n",
    "    - **prepruning**: halt tree construction early\n",
    "    - **postpruning**: remove branches from a \"fully-grown\" tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tree Pruning Example\n",
    "\n",
    "![Tree Pruning Example](./img/8.3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Classification\n",
    "\n",
    "- A statistical classifier:\n",
    "    - predicts class membership probabilities\n",
    "- Foundation: based on Bayes' Theorem\n",
    "- Performance (naive Bayesian classifier)\n",
    "    - comparable to decision tree & some neural network classifiers\n",
    "- Incremental"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayesian Classifier\n",
    "\n",
    "- $X = (x_1,x_2,\\ldots,x_n)$ (i.e., $n$ attributes)\n",
    "- $m$ classes: $C_1,C_2,\\ldots,C_m$\n",
    "- **Classification: maximal $P(C_i | X)$**\n",
    "- Based on Bayes' Theorem $\\quad\\quad \\displaystyle P(C_i|X) = \\frac{P(X|C_i)P(C_i)}{P(X)}$\n",
    "- Since $P(X)$ is constant for all classes, only need to maximize $P(X|C_i)P(C_i)$\n",
    "- **Naive assumption**: class conditional independence (no dependence between attributes)\n",
    "    - $\\displaystyle P(X|C_i) = \\prod_{k=1}^n P(x_k|C_i) = P(x_1|C_i) \\times P(x_2|C_i) \\times \\cdots \\times P(x_n|C_i)$\n",
    "- If $A_k$ is categorical, $P(x_k|C_i)$\n",
    "- If $A-k$ is continuous-valued, assume Gaussian distribution\n",
    "    - $\\displaystyle P(x_k|C_i) = g(x_k,\\mu_{C_i},\\sigma_{C_i}) = \\frac{1}{\\sqrt{2\\pi}\\sigma}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}$\n",
    "- **Advantage**\n",
    "    - easy to compute, good results in most cases\n",
    "- **Disadvantage**\n",
    "    - assumption: class conditional independence\n",
    "    - dependencies exist in practice\n",
    "        - e.g., hospital patients: age, family history, fever, cough, lung cancer, diabetes, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayesian Classifier Example\n",
    "\n",
    "- 2 classes: buys_computer\n",
    "    - $C_1$: yes\n",
    "    - $C_2$: no\n",
    "- X = (age $\\leq$ 30, income=medium, student=yes, credit_raiting=fair)\n",
    "- $P(C_i|X) = P(X|C_i)P(C_i)/P(X)$\n",
    "    - $P(X|C_i)$ = P(age $\\leq$ 30 | $C_i$)\n",
    "        - P(income-medium | C_i)\n",
    "        - P(student=yes | C_i)\n",
    "        - P(credit_rating=fair | C_i)\n",
    "\n",
    "![Naive Bayesian Classifier Example](./img/8.4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Avoid 0-Probability\n",
    "\n",
    "- The 0-probability problem\n",
    "    - e.g., income: low(0), medium(990), high(10)\n",
    "- Laplacian correction (or Laplace estimator)\n",
    "    - add 1 to each case: non-zero, close to original probabilities\n",
    "    - e.g., income: low(1), medium(991), high(11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IF-THEN Rules\n",
    "\n",
    "- IF condition THEN condition\n",
    "    - R: IF age = youth AND student = yes THEN buys_computer = yes\n",
    "    - rule antecedent/precondition (IF) rule consequent (THEN)\n",
    "- Assessment of a rule\n",
    "    - **coverage**(R) = $n_{\\text{covers}} / |D| \\quad\\quad$ **accuracy**(R) = $n_{\\text{correct}} / n_{\\text{covers}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule Assessment Example\n",
    "\n",
    "- R: IF age $\\leq$ 30 AND student = yes THEN buys_computer = yes\n",
    "- coverage (R) = 2/14\n",
    "- accuracy (R) = 2/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule-Based Classification\n",
    "\n",
    "- Rule R is **triggered** (precondition satisfied)\n",
    "- R is the only rule triggered\n",
    "- No rule is triggered\n",
    "- More than one rules are triggered\n",
    "- **size ordering**: most attribute tests\n",
    "- **class-based ordering**: importance (e.g., prevalence, misclassification cost)\n",
    "- **rule-based ordering**: (decision list) priority list ordered by rule quality or by experts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule Extraction\n",
    "\n",
    "- From a decision tree\n",
    "- Each root to leaf path\n",
    "- Leaf: class prediction\n",
    "- Rules are exhaustive and mutually exclusive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier Accuracy Measures\n",
    "\n",
    "- Partition: training data and testing data\n",
    "- Accuracy, recognition rate\n",
    "- Error rate, misclassification rate\n",
    "- **Confusion matrix**\n",
    "\n",
    "![Confusion matrix](./img/8.5.png)\n",
    "\n",
    "- **Sensitivity**: t_pos / pos\n",
    "- **Specificity**: t_net / neg\n",
    "- **Precision**: t_pos / (t_pos + f_pos)\n",
    "- $\\displaystyle accuracy = sensitivity\\frac{pos}{pos + neg} + specificity\\frac{neg}{pos + neg}$\n",
    "- Costs and benefits of TP,TN,FP,FN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier/Predictor Evaluation\n",
    "\n",
    "- **Holdout, random sampling**\n",
    "\n",
    "![Classifier predictor evaluation](./img/8.6.png)\n",
    "\n",
    "- **Cross-validation**\n",
    "    - divide into $k$ subsamples\n",
    "    - use $k-1$ subsamples for training, one for testing, --- k-fold cross validation\n",
    "- **Bootstrapping** (e.g.,.632 bootstrapping)\n",
    "    - sample with replacement $\\Rightarrow$ training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictor Error Measures\n",
    "\n",
    "Absolute error: $\\displaystyle |y_i - y_i'|$\n",
    "\n",
    "Square error: $\\displaystyle (y_i - y_i')^2$\n",
    "\n",
    "Mean absolute error: $\\displaystyle \\frac{\\sum_{i=1}^d |y_i - y_i'|}{d}$\n",
    "\n",
    "Mean square error: $\\displaystyle \\frac{\\sum_{i=1}^d (y_i - y_i')^2}{d}$\n",
    "\n",
    "Relative absolute error: $\\displaystyle \\frac{\\sum_{i=1}^d |y_i - y_i'|}{\\sum_{i=1}^d |y_i - \\bar{y}|}$\n",
    "\n",
    "Relative square error: $\\displaystyle \\frac{\\sum_{i=1}^d (y_i - y_i')^2}{\\sum_{i=1}^d (y_i - \\bar{y})^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "- Choose between two models $M_1$ and $M_2$\n",
    "- Mean error rate? (k-fold cross validation)\n",
    "    - estimated error on future data\n",
    "- Difference between error rates of $M_1$ & $M_2$\n",
    "    - statistically significant? or by chance?\n",
    "    - $\\displaystyle t = \\frac{\\overline{err}(M_1) - \\overline{err}(M_2)}{\\sqrt{var(M_1 - M_2)/k}}$\n",
    "- **t-test**\n",
    "    - $\\displaystyle var(M_1 - M_2) = \\frac{1}{k}\\sum_{i=1}^k \\left[ err(M_1)_i - err(M_2)_i - (\\overline{err}(M_1) - \\overline{err}(M_2)) \\right]^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T-test Example\n",
    "\n",
    "- 10-fold cross-validation: 10 pairs of error values\n",
    "- T-table (e.g., [https://www.itl.nist.gov/div898/handbook/eda/section3/eda3672.htm](https://www.itl.nist.gov/div898/handbook/eda/section3/eda3672.htm))\n",
    "- Degree of freedom: $v = 10 - 1 = 9$\n",
    "- Significance level: $a = 0.05$\n",
    "- Two-sided test: $1 - a/2 = 1 - 0.05/2 = 0.975$\n",
    "- Check T-table $(v=9, 0.975)$: critical value is $2.262$\n",
    "- Compute $t$, statistically significant if $t > 2.262$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection: ROC Curves\n",
    "\n",
    "- Visual comparison of models\n",
    "- X: **false positive rate**\n",
    "    - f_pos / neg\n",
    "- Y: **true positive rate**\n",
    "    - t_pos / pos\n",
    "- **Area below curve**:\n",
    "    - accuracy, diagonal line: 0.5 accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Methods\n",
    "\n",
    "- Use a combination of multiple models to increase accuracy\n",
    "- Popular ensemble methods\n",
    "    - **bagging**: equal-weight votes\n",
    "    - **boosting**: weighted votes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging: Bootstrap Aggregation\n",
    "\n",
    "- Analogy: diagnosis by multiple doctors\n",
    "    - multiple doctors' **majority vote**\n",
    "- Training: given a data set $D$ of $d$ tuples\n",
    "    - training set $D_i$: $d$ tuples sampled randomly with replacement (i.e., bootstrap)\n",
    "    - classifier $M_i$ is learned for $D_i$\n",
    "- Classification: majority vote\n",
    "- Prediction: average of multiple predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "\n",
    "- Analogy: diagnosis by multiple doctors\n",
    "    - **weighted** by previous diagnosis accuracy\n",
    "- Weights are assigned to each training tuple\n",
    "- A series of $k$ classifiers is iteratively learned\n",
    "- After classifier $M_i$ is learned, adjust weights so $M_{i+1}$ pays more attention to tuples that were misclassified by $M_i$\n",
    "- $M^*$ combines the votes of all $k$ classifiers, weighted by individual accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaboost\n",
    "\n",
    "- $D: (X_1,y_1),(X_2,y_2),\\ldots,(X_d,y_d)$\n",
    "- Initial weight of each tuple: $1/d$\n",
    "- Round $i\\ (i=1,\\ldots,k) \\quad\\quad \\displaystyle error(M_i) = \\sum_{j=1}^d w_j \\times err(X_j)$\n",
    "    - $D_i$ sample $d$ tuples w/ replacement from $D$\n",
    "    - Pr(choose $\\text{tuple}_j$) based on $\\text{tuple}_j$'s weight\n",
    "    - Learn $M_i$ from $D_i$, compute its error rate\n",
    "    - Reduce weights of correctly classified tuples\n",
    "        - $\\displaystyle w_j = w_j \\times \\frac{error(M_i)}{1 - error(M-i)}$\n",
    "    - Normalize tuple weights so sum is 1.0\n",
    "        - $\\displaystyle weight(M_i) = \\log\\frac{1 - error(M_i)}{error(M-i)}$\n",
    "- Classification: weighted votes of $k$ classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble Methods: Accuracy\n",
    "\n",
    "- **Bagging**\n",
    "    - often significantly better than single classifier\n",
    "    - noise: not considerably worse, more robust\n",
    "    - prediction: proved improved accuracy\n",
    "- **Boosting**\n",
    "    - generally better than bagging\n",
    "    - may overfit the model to misclassified data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
