{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measures of Data Quality\n",
    "\n",
    "- Accuracy\n",
    "- Completeness\n",
    "- Consistency\n",
    "- Timeliness\n",
    "- Believability\n",
    "- Interpretability\n",
    "- Accessibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Major Tasks in Preprocessing\n",
    "\n",
    "- **Data cleaning**\n",
    "    - fill in missing values, smooth noisy data, identify or remove outliers, resolve inconsistencies\n",
    "- **Data integration**\n",
    "    - integration of multiple data sources\n",
    "- **Data reduction**\n",
    "     - dimensionality, numerosity, compression\n",
    "- **Data transformation and data discretization**\n",
    "    - normalization, concept hierarchy generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Data Cleaning?\n",
    "\n",
    "- Imperfect real-world data\n",
    "- **Incomplete**: missing attributes, values\n",
    "    - e.g., age = \"\", major = \"\"\n",
    "- **Noisy**: containing errors or outliers\n",
    "    - e.g., salary = \"-10\"\n",
    "- **Inconsistent**: containing discrepancies\n",
    "    - e.g., age = \"21\", birthday = \"08/03/1995\"\n",
    "    - e.g., ratings of \"1, 2, 3\" and \"A, B, C\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why Are Data Imperfect?\n",
    "\n",
    "- **Incomplete data**\n",
    "    - \"not applicable\" values\n",
    "    - time between collection and analysis\n",
    "    - human/hardware/software problems\n",
    "- **Noisy data**\n",
    "    - faulty data collection instruments\n",
    "    - human or computer error at data entry\n",
    "    - errors in data transmission\n",
    "- **Inconsistent data**\n",
    "    - different data sources\n",
    "    - naming conventions, data formats\n",
    "        - e.g., data \"03/07/11\"\n",
    "    - functional dependency violation\n",
    "        - e.g., modify some linked data\n",
    "- **No quality data, no quality data mining results!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Handle Noisy Data?\n",
    "\n",
    "- **Binning**\n",
    "    - first sort & partition data into bins\n",
    "    - then smooth by\n",
    "        - bin means\n",
    "        - bin median\n",
    "        - bin boundaries\n",
    "\n",
    "![Image: Binning](img/3.1.png)\n",
    "\n",
    "- **Regression**\n",
    "    - fit data into regression functions\n",
    "\n",
    "![Image: Regression](img/3.2.png)\n",
    "\n",
    "- **Clustering**\n",
    "    - detect and remove outliers\n",
    "\n",
    "![Image: Clustering](img/3.3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Integration\n",
    "\n",
    "- Combines data from multiple sources\n",
    "- **Entity identification**\n",
    "    - schema integration, object matching\n",
    "    - e.g., student_id vs. student_number\n",
    "- **Redundant data**\n",
    "    - different naming, derived data\n",
    "    - may be detected by correlation analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Analysis\n",
    "\n",
    "- **Correlation coefficient (numerical data)**\n",
    "    - $\\displaystyle r_{A,B} = \\frac{\\sum_{i=1}^N(a_i-\\bar{A})(b_i-\\bar{B})}{N\\sigma_A\\sigma_B} = \\frac{\\sum_{i=1}^N(a_ib_i)-N\\bar{A}\\bar{B}}{N\\sigma_A\\sigma_B}$\n",
    "- **$X^2 (chi-square) test (categorical data)**\n",
    "    - $\\displaystyle \\chi^2 = \\sum_{i=1}^c\\sum_{j=1}^r\\frac{(o_{ij}-e_{ij})^2}{e_{ij}}$\n",
    "    - $\\displaystyle e_{ij} = \\frac{count(A=a_i) \\times count(B=b_j)}{N}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chi-Square Test: An Example\n",
    "\n",
    "||play chess|not play chess|total|\n",
    "|---|---|---|---|\n",
    "|like fiction|250 (**90**)|200 (**360**)|450|\n",
    "|not like fiction|50 (**210**)|1000 (**840**)|1050|\n",
    "|total|300|1200|1500|\n",
    "\n",
    "$\\displaystyle e_{11} = \\frac{\\text{\\#(like fiction)} \\times \\text{\\#(play chess)}}{N} = \\frac{450 \\times 300}{1500} = 90$\n",
    "\n",
    "$\\displaystyle \\chi^2 = \\frac{(250 - 90)^2}{90} + \\frac{(50 - 210)^2}{210} + \\frac{(200 - 360)^2}{360} + \\frac{(1000 - 840)^2}{840} = 507.93$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Analysis\n",
    "\n",
    "- **Correlation coefficient**\n",
    "    - numeric data, $[-1.0, 1.0]$\n",
    "- **$\\chi^2$ (chi-square) test**\n",
    "    - categorical data, $\\geq 0$\n",
    "    - $d = (c-1)(r-1)$\n",
    "- **Correlation vs. causality**\n",
    "    - Does **correlation** imply **causality**?\n",
    "        - sleeping with one's shoes on is strongly correlated with waking up with a headache\n",
    "        - the more fireman fighting a damage, the more damage there is going to be\n",
    "        - as ice cream sales increase, the rate of drowning deaths increases sharply\n",
    "        - **correlation does not imply causality!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Reduction\n",
    "\n",
    "- **Why data reduction?**\n",
    "    - massive data sets\n",
    "    - mining takes a long time\n",
    "- **Goal of data reduction**\n",
    "    - data set is much smaller in volume\n",
    "    - produces (almost) the same mining results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Reduction Strategies\n",
    "\n",
    "- **Dimensionality reduction**\n",
    "    - attribute subset selection\n",
    "    - Wavelet transform\n",
    "    - principle component analysis (PCA)\n",
    "- **Numerosity reduction**\n",
    "    - regression, log-linear models\n",
    "    - data cube aggregation\n",
    "    - histograms, clustering, sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attribute Subset Selection\n",
    "\n",
    "- Remove irrelevant or redundant attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction\n",
    "\n",
    "- **Discrete wavelet transform (DWT)**\n",
    "    - linear signal processing, multi-resolution\n",
    "    - store a small fraction of the strongest wavelet coefficients\n",
    "- **Principal component analysis (PCA)**\n",
    "    - given $N$ data vectors of $n$ dimensions\n",
    "    - find $k \\leq n$ orthogonal vectors (principal components) that can\n",
    "    - best represent the data\n",
    "    - for numerical data only\n",
    "    - used when $n$ is large"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerosity Reduction\n",
    "\n",
    "- Use alternative, smaller data representations\n",
    "- **Parametric methods**\n",
    "    - assume the data fits some model\n",
    "    - estimate model parameters\n",
    "    - store the parameters, discard the data\n",
    "- **Non-parametric methods**\n",
    "    - do not assume models\n",
    "    - e.g., histograms, clustering, sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression & Log-Linear Models\n",
    "\n",
    "- **Linear regression**\n",
    "    - $Y = wX + b$\n",
    "- **Multiple regression**\n",
    "    - $Y = b_0 + b_1X_1 + b_2X_1$\n",
    "- **Log-linear models**\n",
    "    - approximate multi-dimensional probability distributions with lower-dimensional distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cube Aggregation\n",
    "\n",
    "- E.g., quarterly sales $\\Rightarrow$ annual sales\n",
    "- Multiple levels of aggregation may be possible\n",
    "- Use the smallest representation which is enough for the task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Histograms\n",
    "\n",
    "- Divide data into buckets and store average (or sum) for each bucket\n",
    "- Partitioning rules\n",
    "    - equal-width\n",
    "    - equal-frequency\n",
    "    - v-optimal\n",
    "    - max-diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling\n",
    "\n",
    "- Use a small sample to represent whole data\n",
    "- Choose a **representative** subset of the data\n",
    "    - simple random sampling may have very poor performance in the presence of skew\n",
    "- Simple random sample without replacement\n",
    "- Simple random sample with replacement\n",
    "- Cluster sample\n",
    "- Stratified sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample w/ or w/o Replacement\n",
    "\n",
    "![Sample w or wo Replacement](./img/3.4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster or Stratified Sampling\n",
    "\n",
    "- Approximate the percentage of each class\n",
    "\n",
    "![Cluster or Stratified Sampling](./img/3.5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation\n",
    "\n",
    "- **Smoothing**: remove noise from data\n",
    "- **Aggregation**: summarization\n",
    "    - e.g., daily sales $\\Rightarrow$ monthly, annual sales\n",
    "- **Generalization**: concept hierarchy climbing\n",
    "    - e.g., street $\\Rightarrow$ city $\\Rightarrow$ state\n",
    "- **Normalization**: scale to fall within a range\n",
    "- **Attribute/feature construction**: new attributes constructed from existing ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization\n",
    "\n",
    "- **Min-max normalization**\n",
    "    - $\\displaystyle v' = \\frac{v-\\min_A}{\\max_A - \\min_A}(\\text{new\\_max}_A - \\text{new\\_min}_A) + \\text{new\\_min}_A$\n",
    "    - e.g., income range $[\\$12000,\\$98000]$, normalize to $[0.0,1.0]$ then $73,600 is mapped to\n",
    "        - $\\displaystyle \\frac{73600 - 12000}{98000 - 12000}(1.0 - 0) + 0 = 0.716$\n",
    "- **Z-score normalization**\n",
    "    - e.g., mean = 54,000\n",
    "    - stdev = 16,000\n",
    "    - then\n",
    "        - $\\displaystyle \\frac{73600 - 54000}{16000} = 1.225$\n",
    "- **Normalization by decimal scaling**\n",
    "    - $\\displaystyle v' = \\frac{v}{10^j}$\n",
    "    - where $j$ is the smallest integer s.t. $\\max(|v'|) < 1$\n",
    "    - e.g., range $[-986,917]$\n",
    "        - $j=3$, divide by 1000\n",
    "        - $-986 \\Rightarrow -0.986$\n",
    "        - $917 \\Rightarrow 0.917$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discretization\n",
    "\n",
    "- Three types of attributes\n",
    "    - **nominal**: unordered set (e.g., profession)\n",
    "    - **ordinal**: ordered set (e.g., military rank)\n",
    "    - **continuous**: e.g., integer or real numbers\n",
    "- Discretization\n",
    "    - divide continuous range into intervals\n",
    "    - interval labels used to replace data values\n",
    "    - supervised vs. unsupervised, split vs. merge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discretization Methods\n",
    "\n",
    "- **Binning**: split, unsupervised\n",
    "- **Histogram analysis**: split, unsupervised\n",
    "- **Clustering analysis**: split/merge, unsupervised\n",
    "- **Entropy-based discretization**: split, supervised\n",
    "- **Interval merging by $X^2$ analysis**: merge, supervised\n",
    "- **Intuitive partitioning**: split, unsupervised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy-Based Discretization\n",
    "\n",
    "- Partition $D$ into $D_1$ and $D_2$ at boundary $A$\n",
    "- Pick boundary $A$ with minimum $Info_A(D)$\n",
    "    - \"purer\" distribution has lower entropy\n",
    "- Apply recursively to each partition\n",
    "- Top-down split, supervised (uses class info)\n",
    "\n",
    "$\\displaystyle Info_A(D) = \\frac{|D_1|}{|D|}Entropy(D_1) + \\frac{|D_2|}{|D|}Entropy(D_2)$\n",
    "\n",
    "$\\displaystyle Entropy(D_1) = -\\sum_{i=1}^m p_i \\log_2(p_i)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interval Merge by $X^2$ Analysis\n",
    "\n",
    "- Bottom-up merge, supervised\n",
    "- Merge the best neighboring intervals\n",
    "    - interval w/ most similar class distributions\n",
    "- **ChiMerge**\n",
    "    - merge adjacent intervals w/ min $X^2$ value\n",
    "        - i.e., class is independent of interval\n",
    "    - stopping criterion\n",
    "        - significance, #intervals, inconsistency, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concept Hierarchy Generation\n",
    "\n",
    "- Categorical data\n",
    "- Partial/total ordering of attributes\n",
    "    - street < city < state < country\n",
    "- Automatic concept hierarchy generation\n",
    "- fewer distinct values $\\Rightarrow$ higher level\n",
    "- e.g., street, city, state, country\n",
    "- exceptions\n",
    "    - e.g., weekday, month, quarter, year"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
